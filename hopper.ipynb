{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import tqdm\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.logger import Video\n",
    "import tensorboard\n",
    "\n",
    "from utils import gauss, gaussp, inv_hea, hea, porte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        # Access relevant state and action variables\n",
    "        \n",
    "        #state\n",
    "        mapping = {\"z\": 0, \"a\": 1, \"a_hip\": 2, \"a_knee\": 3, \"a_ankle\": 4, \"v_x\": 5, \"v_z\": 6, \"a_d\": 7, \"a_hip_d\": 8, \"a_knee_d\": 9, \"a_ankle_d\": 10}\n",
    "        z = obs[0]                  # z of the robot\n",
    "        a = obs[1]                  # Angle of the robot\n",
    "        a_hip = obs[2]              # Angle of the hip\n",
    "        a_knee = obs[3]             # Angle of the knee\n",
    "        a_ankle = obs[4]            # Angle of the ankle\n",
    "        v_x = obs[5]                # Velocity in x direction\n",
    "        v_z = obs[6]                # Velocity in z direction\n",
    "        a_d = obs[7]                # Angular velocity \n",
    "        a_hip_d = obs[8]            # Angular velocity of the hip\n",
    "        a_knee_d = obs[9]           # Angular velocity of the knee\n",
    "        a_ankle_d = obs[10]         # Angular velocity of the ankle\n",
    "        \n",
    "        #action\n",
    "        torque_hip = action[0]      # Torque applied to the hip\n",
    "        torque_knee = action[1]     # Torque applied to the knee\n",
    "        torque_ankle = action[2]    # Torque applied to the ankle\n",
    "\n",
    "        energy_used = np.sum(np.square(action))  # Simplistic energy calculation\n",
    "        energy_kin = (a_d**2 + a_hip_d**2 + a_knee_d**2 + a_ankle_d**2 + v_x**2 + v_z**2)\n",
    "\n",
    "        # Custom reward logic\n",
    "        get_straight = gauss(z, 1.2, 0.3*1.2) * gauss(a, -2*np.pi, 0.30*2*np.pi) * gauss(a_hip, 0, 0.30*2*np.pi) * gauss(a_knee, 0, 0.30*2*np.pi) * gauss(a_ankle, 0, 0.30*2*np.pi)\n",
    "        vel_flip = - 2 * torque_hip * a_hip + torque_knee * a_knee + torque_ankle * a_ankle\n",
    "        \n",
    "        flipping = - a_d * (1.0 + 18 * z + .6 * vel_flip - .8 * v_x) - 15 * abs(a_hip)\n",
    "        landing = 5e2 * get_straight #huge reward for good landing\n",
    "        \n",
    "        if -a > 2 * np.pi - np.pi/3 and -a <  2 * np.pi + np.pi/3:\n",
    "            flipping = 0\n",
    "            landing *= (1 - max(1, landing/(.9 * energy_kin + 9e1 * energy_used)))\n",
    "        \n",
    "        bonus = 500\n",
    "        conds = [z > 0.95, -a > 2 * np.pi - np.pi/3, -a <  2 * np.pi + np.pi/3]\n",
    "        if all(conds):\n",
    "            flipping = 0\n",
    "            landing += bonus\n",
    "            \n",
    "        custom_reward = 0\n",
    "        custom_reward += landing\n",
    "        custom_reward += flipping\n",
    "        \n",
    "        # attributes for monitoring\n",
    "        self.vel_act = vel_flip\n",
    "        self.get_straight = get_straight\n",
    "        self.landing = landing\n",
    "        self.flipping = flipping\n",
    "\n",
    "        if done:\n",
    "            custom_reward -= 5e3  # Heavy penalty for falling\n",
    "        return obs, custom_reward, done, truncated, info  \n",
    "    \n",
    "class VideoRecorderCallback(BaseCallback):\n",
    "    def __init__(self, eval_env: gym.Env, render_freq: int, n_eval_episodes: int = 1, deterministic: bool = True):\n",
    "        \"\"\"\n",
    "        Records a video of an agent's trajectory traversing ``eval_env`` and logs it to TensorBoard\n",
    "\n",
    "        :param eval_env: A gym environment from which the trajectory is recorded\n",
    "        :param render_freq: Render the agent's trajectory every eval_freq call of the callback.\n",
    "        :param n_eval_episodes: Number of episodes to render\n",
    "        :param deterministic: Whether to use deterministic or stochastic policy\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._eval_env = eval_env\n",
    "        self._render_freq = render_freq\n",
    "        self._n_eval_episodes = n_eval_episodes\n",
    "        self._deterministic = deterministic\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._render_freq == 0:\n",
    "            screens = []\n",
    "\n",
    "            def grab_screens(_locals, _globals) -> None:\n",
    "                \"\"\"\n",
    "                Renders the environment in its current state, recording the screen in the captured `screens` list\n",
    "\n",
    "                :param _locals: A dictionary containing all local variables of the callback's scope\n",
    "                :param _globals: A dictionary containing all global variables of the callback's scope\n",
    "                \"\"\"\n",
    "                screen = self._eval_env.render()\n",
    "                # PyTorch uses CxHxW vs HxWxC gym (and tensorflow) image convention\n",
    "                screens.append(screen.transpose(2, 0, 1))\n",
    "\n",
    "            evaluate_policy(\n",
    "                self.model,\n",
    "                self._eval_env,\n",
    "                callback=grab_screens,\n",
    "                n_eval_episodes=self._n_eval_episodes,\n",
    "                deterministic=self._deterministic,\n",
    "            )\n",
    "            self.logger.record(\n",
    "                \"video render\",\n",
    "                value=Video(th.from_numpy(np.asarray([screens])), fps=50),\n",
    "                exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "            )\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/killian/Documents/code/hopper'\n",
    "d = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "healthy_reward = 1\n",
    "healthy_z_range = (0.2, 1.25)\n",
    "healthy_angle_range = (-float(\"inf\"), float(\"inf\"))\n",
    "reset_noise_scale = 5e-3\n",
    "n_learning_steps = 5_000_000\n",
    "\n",
    "env = gym.make('Hopper-v4', render_mode='rgb_array', healthy_reward=healthy_reward, healthy_z_range=healthy_z_range, healthy_angle_range=healthy_angle_range, reset_noise_scale=reset_noise_scale)\n",
    "\n",
    "video_recorder = VideoRecorderCallback(env, render_freq=50_000)\n",
    "env = CustomRewardWrapper(env)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"../Tensorboard/\")\n",
    "\n",
    "model_load = False\n",
    "d_load = \"2024-08-14_17-41-23\"\n",
    "n_learning_steps_load = 1_000_000\n",
    "if model_load:\n",
    "    d = d_load\n",
    "    env.reset()\n",
    "    file = folder + \"/Model/hopper_model_%s\"%d\n",
    "    model = PPO.load(file, env=env, verbose=1, tensorboard_log=\"../Tensorboard/\")\n",
    "    model.learn(total_timesteps=n_learning_steps_load, callback=video_recorder, tb_log_name=d)\n",
    "else:\n",
    "    model.learn(total_timesteps=n_learning_steps, callback=video_recorder, tb_log_name=d)\n",
    "\n",
    "# Save the model\n",
    "model.save(folder + \"/Model/hopper_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "\n",
    "# Action and observation mapping\n",
    "mapping = {\"z\": 0, \"a\": 1, \"a_hip\": 2, \"a_knee\": 3, \"a_ankle\": 4, \"v_x\": 5, \"v_z\": 6, \"a_d\": 7, \"a_hip_d\": 8, \"a_knee_d\": 9, \"a_ankle_d\": 10}\n",
    "\n",
    "# Set up video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('hopper-flip.mp4', fourcc, 50, (480, 480))\n",
    "\n",
    "N_step = 1000\n",
    "s_a = np.zeros((N_step, 14))  # 11 states + 3 actions\n",
    "rwd = np.zeros((N_step, 4))   # vel_act, get_straight, landing, flipping\n",
    "\n",
    "energy_kin = []\n",
    "energy_used = []\n",
    "\n",
    "for i in range(N_step):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    img = vec_env.render(mode='rgb_array')\n",
    "    \n",
    "    energy_kin.append(obs[0][5]**2 + obs[0][6]**2 + obs[0][7]**2 + obs[0][8]**2 + obs[0][9]**2 + obs[0][10]**2)\n",
    "    energy_used.append(np.sum(np.square(action)))\n",
    "    \n",
    "    # Convert RGB to BGR for OpenCV compatibility\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    out.write(img)\n",
    "    \n",
    "    s_a[i, :] = np.concatenate((obs[0], action[0]))\n",
    "    rwd[i, :] = [env.vel_act, env.get_straight, env.landing, env.flipping]\n",
    "    \n",
    "    if done:\n",
    "        N_stop = i\n",
    "        print(f\"Episode finished after {i+1} timesteps\")\n",
    "        break\n",
    "        obs = vec_env.reset()\n",
    "\n",
    "out.release()\n",
    "\n",
    "plt.figure(\"energy_kin\")\n",
    "plt.plot(0.9 * np.array(energy_kin), label=\"energy_kin\")\n",
    "plt.plot(90 * np.array(energy_used), label=\"energy_used\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Truncate the arrays to only include the steps before termination\n",
    "s_a = s_a[:N_stop, :]\n",
    "rwd = rwd[:N_stop, :]\n",
    "\n",
    "# Plotting function\n",
    "def plot(map: str=\"z\"):\n",
    "    plt.figure()\n",
    "    plt.plot(s_a[:, mapping[map]], label=map)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all observations\n",
    "for key in mapping.keys():\n",
    "    plot(key)\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure(\"reward\")\n",
    "plt.plot(rwd[:, 0], label=\"vel_act\")\n",
    "plt.plot(rwd[:, 1], label=\"get_straight\")\n",
    "plt.plot(rwd[:, 2], label=\"landing\")\n",
    "plt.plot(rwd[:, 3], label=\"flipping\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#folder = 'c:/Users/killi/Documents/code/Hopper-4-flip'\n",
    "folder = '/home/killian/Documents/code/hopper'\n",
    "\n",
    "save_folders = ['/Model', '/Tensorboard', '/Render', '/Run']\n",
    "for save_folder in save_folders:\n",
    "    try: \n",
    "        os.mkdir(folder + save_folder)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "shutil.copyfile('hopper.ipynb', folder + '/Run/hopper_%s.ipynb'%(d))\n",
    "shutil.copyfile('hopper-flip.mp4', folder + '/Render/hopper-flip_%s.mp4'%(d))\n",
    "shutil.copyfile('hopper_model.zip', folder + '/Model/hopper_model_%s.zip'%(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
